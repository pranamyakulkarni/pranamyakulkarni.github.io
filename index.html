<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Pranamya Kulkarni</title>
    <meta name="author" content="Pranamya Kulkarni">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          
          <!-- PROFILE SECTION -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Pranamya Kulkarni
                </p>
                <p>
                  I am a Pre-Doctoral Researcher at Google DeepMind India, working in the Machine Learning and Optimization (MLO) team. I work with <a href="https://www.prateekjain.org/">Dr. Prateek Jain</a>, <a href="https://sites.google.com/corp/site/pshenoyuw/">Dr. Pradeep Shenoy</a>, <a href="https://sites.google.com/corp/view/karthikeyan-shanmugam">Dr. Karthikeyan Shanmugam</a>, and <a href="https://www.cs.cmu.edu/~asuggala/">Dr. Arun Suggala</a>. My current research focuses on improving the <strong>training and inference efficiency</strong> of Large Language Models, as well as enabling <strong>label-free robotic pose estimation</strong> using causal representation learning. 
                </p>
                <p>
                  Previously, I completed an Interdisciplinary Dual Degree at <a href="https://www.iitb.ac.in/">IIT Bombay</a>, earning a B.Tech (Honors) in Electrical Engineering and an M.Tech in Artificial Intelligence and Data Science. I was advised by <a href="https://www.cse.iitb.ac.in/~soumen/">Prof. Soumen Chakrabarti</a> and <a href="https://abir-de.github.io/">Prof. Abir De</a>, where I worked on Graph Neural Networks. I also completed a research internship at Adobe Research India.
                </p>
                <p>
                  In my free time, I enjoy playing and watching cricket, playing chess, and gaming on PlayStation.
                </p>
                <p style="text-align:center">
                  <!-- Just display the email address as text -->
                  Email: pranamyakulkarni@gmail.com &nbsp;/&nbsp;
                  <a href="data/Pranamya_Resume.pdf">Resume</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=6SmRwUgAAAAJ&hl=en">Scholar</a>
                  <!-- <a href="https://github.com/pranamyakulkarni">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <!-- ☆ YOUR PHOTO HERE -->
                <img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;" alt="profile photo" src="images/PranamyaKulkarni_SecurityPhoto.png">
              </td>
            </tr>
          </tbody></table>
          
          <!-- PROJECTS SECTION -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <!-- =============== START OF PROJECT 4 (MemCom) =============== -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/memcom_image.png" alt="MemCom project thumbnail" style="width:160px;border-radius:8px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2510.16092">
                  <span class="papertitle">Compressing Many-Shots in In-Context Learning</span>
                </a>
                <br>
                <a href="https://www.devvrit.com/">Devvrit Khatri</a>, <strong>Pranamya Kulkarni</strong>, <a href="https://nilesh2797.github.io/">Nilesh Gupta</a>, <a href="https://varun221.github.io/">Yerram Varun</a>, <a href="https://scholar.google.com/citations?user=WnzIgp8AAAAJ&hl=en">Liqian Peng</a>, <a href="https://research.google/people/author36197/">Jay Yagnik</a>, <a href="https://praneethnetrapalli.org/">Praneeth Netrapalli</a>, <a href="https://web.cs.ucla.edu/~chohsieh/">Cho-Jui Hsieh</a>, <a href="https://scholar.google.com/citations?hl=en&user=I54RNh4AAAAJ&view_op=list_works&sortby=pubdate">Alec Go</a>, <a href="https://www.cs.utexas.edu/~inderjit/">Inderjit S Dhillon</a>, <a href="https://adityakusupati.com/">Aditya Kusupati</a>, <a href="https://www.prateekjain.org/">Pratek Jain</a>
                <br>
                <em>Preprint available on arXiv</em>
                <br>
                <a href="https://arxiv.org/abs/2510.16092">arXiv</a>
                <p>
                We introduce MemCom, a layer-wise compression method designed to improve the memory and computational efficiency of many-shot In-Context Learning (ICL). By compressing prompts into a compact key-value (KV) cache, MemCom significantly reduces inference costs while maintaining high performance across various classification tasks.
                </p>
              </td>
            </tr>
            <!-- =============== END OF PROJECT 4 =============== -->

            <!-- =============== START OF PROJECT 3 (ROPES) =============== -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ROPES_image.png" alt="ROPES project thumbnail" style="width:160px;border-radius:8px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2510.20884">
                  <span class="papertitle">ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning</span>
                </a>
                <br>
                <strong>Pranamya Kulkarni</strong>*, <a href="https://puranjay14.github.io/puranjaydatta/">Puranjay Datta</a>*, <a href="https://scholar.google.com/citations?user=KLiI1JwAAAAJ&hl=en">Emre Acartürk</a>, <a href="https://bvarici.github.io/">Burak Varıcı</a>, <a href="https://sites.google.com/corp/view/karthikeyan-shanmugam">Karthikeyan Shanmugam</a>, <a href="https://www.isg-rpi.com/">Ali Tajer</a>
                <br>
                <em>Accepted to the Embodied World Models for Decision Making Workshop at NeurIPS 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2510.20884">arXiv</a>
                <p>
                We propose ROPES, an unsupervised framework for robotic pose estimation that recovers joint angles from raw images using score-based Causal Representation Learning. By leveraging interventional data distributions, ROPES disentangles controllable latent variables without requiring any explicit pose labels.
                </p>
              </td>
            </tr>
            <!-- =============== END OF PROJECT 3 =============== -->

            <!-- =============== START OF PROJECT 1 (M3CoL) =============== -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/M3col_image.png" alt="M3CoL project thumbnail" style="width:160px;border-radius:8px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://raghavsinghal10.github.io/m3col_page/"><span class="papertitle">M3CoL: Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification</span></a>
                <br>
                <a href="https://raja-7-c.github.io/">Raja Kumar</a>*, <a href="https://raghavsinghal10.github.io/">Raghav Singhal</a>*, <strong>Pranamya Kulkarni</strong>, <a href="https://research.monash.edu/en/persons/deval-mehta/">Deval Mehta</a>, <a href="https://www.linkedin.com/in/kshitij-jadhav-64083b110">Kshitij Jadhav</a>
                <br>
                <em>TMLR</em>
                <br>
                <a href="https://raghavsinghal10.github.io/m3col_page/">project page</a> / 
                <a href="https://github.com/RaghavSinghal10/M3CoL">code</a> / 
                <a href="https://arxiv.org/abs/2409.17777">arXiv</a>
                <p>
                We introduce a multimodal mixup-based contrastive learning framework that effectively captures shared relations across modalities, enabling robust multimodal representation learning.
                </p>
              </td>
            </tr>
            <!-- =============== END OF PROJECT 1 =============== -->

            <!-- =============== START OF PROJECT 2 (IndiBias) =============== -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/IndiBias_Image.png" alt="IndiBias project thumbnail" style="width:160px;border-radius:8px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2403.20147"><span class="papertitle">IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context</span></a>
                <br>
                <a href="https://sahoonihar.github.io/">Nihar Ranjan Sahoo</a>, <strong>Pranamya Kulkarni</strong>, <a href="https://scholar.google.com/citations?user=3YOCIDcAAAAJ&hl=en">Narjis Asad</a>, <a href="https://scholar.google.com/citations?user=fGZ5q1oAAAAJ&hl=en">Arif Ahmad</a>, <a href="https://scholar.google.com/citations?user=8qk2ALgAAAAJ&hl=en">Tanu Goyal</a>, <a href="https://research.adobe.com/person/aparna-garimella/">Aparna Garimella</a>, <a href="https://www.cse.iitb.ac.in/~pb/">Pushpak Bhattacharyya</a>
                <br>
                <em>NAACL 2024</em>
                <br>
                <a href="https://arxiv.org/pdf/2403.20147">Paper</a>
                <p>
                We introduce IndiBias, a comprehensive benchmarking dataset designed to evaluate social biases in Large Language Models within the unique socio-cultural nuances of the Indian context. The dataset is built by adapting existing resources and leveraging LLMs to cover diverse biases like gender, religion, caste, and more.
                </p>
              </td>
            </tr>
            <!-- =============== END OF PROJECT 2 =============== -->

            <!-- =============== START OF PROJECT 5 (Image Description Patent) =============== -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/adobe_image.png" alt="Image Description project thumbnail" style="width:160px;border-radius:8px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://patents.google.com/patent/US20250028911A1/en">
                  <span class="papertitle">Image Description Generation with Varying Levels of Detail</span>
                </a>
                <br>
                <strong>Pranamya Kulkarni</strong>*, <a href="https://www.linkedin.com/in/akshay-iyer2211">Akshay Iyer</a>*, <a href="https://kanpard005.github.io/">Kanad Pardeshi</a>*, <a href="https://in.linkedin.com/in/nikunj-goyal-1831b517a">Nikunj Goyal</a>*, <a href="https://apoorvumang.github.io/">Apoorv Saxena</a>, <a href="https://scholar.google.com/citations?user=ppEFe2AAAAAJ&hl=en">Praneetha Vaddamanu</a>, <a href="https://abhilashasancheti.github.io/">Abhilasha Sancheti</a>, <a href="https://scholar.google.com/citations?user=Q4PJyXIAAAAJ&hl=en">Aparna Garimella</a>, <a href="https://scholar.google.com/citations?user=Rr5yTX0AAAAJ&hl=en">Vishwa Vinay</a>
                <br>
                <em>U.S. Patent Application Filed</em>
                <br>
                <a href="https://patents.google.com/patent/US20250028911A1/en">Patent</a>
                <p>
                We present a system for generating image descriptions with controllable levels of detail, catering to diverse user needs from low-level summaries to high-granularity captions. Our approach introduces a novel scoring mechanism and utilizes architectures like DetailBERT and DenseDetail to allow users to explicitly specify the amount of information required in the output.
                </p>
              </td>
            </tr>
            <!-- =============== END OF PROJECT 5 =============== -->
            
          </tbody></table>

          <!-- FOOTER -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template by <a href="https://github.com/jonbarron/jonbarron.github.io">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </tbody></table>
  </body>
</html>